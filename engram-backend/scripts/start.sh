#!/bin/bash

# Engram Startup Script

set -e

echo "üöÄ Starting Engram..."

# Check if Docker is running
if ! docker info > /dev/null 2>&1; then
    echo "‚ùå Docker is not running. Please start Docker and try again."
    exit 1
fi

# Check if .env file exists
if [ ! -f .env ]; then
    echo "‚ö†Ô∏è  .env file not found. Creating from template..."
    cp env.example .env
    echo "üìù Please edit .env file with your configuration before continuing."
    echo "   Required: SECRET_KEY and provider settings"
    exit 1
fi

# Load environment variables
source .env

# Provider configuration
LLM_PROVIDER=${LLM_PROVIDER:-ollama}
EMBEDDING_PROVIDER=${EMBEDDING_PROVIDER:-ollama}
USES_OLLAMA=false
USES_OPENAI=false

if [ "$LLM_PROVIDER" != "ollama" ] && [ "$LLM_PROVIDER" != "openai" ]; then
    echo "‚ùå LLM_PROVIDER must be one of: ollama, openai"
    exit 1
fi

if [ "$EMBEDDING_PROVIDER" != "ollama" ] && [ "$EMBEDDING_PROVIDER" != "openai" ] && [ "$EMBEDDING_PROVIDER" != "local" ]; then
    echo "‚ùå EMBEDDING_PROVIDER must be one of: ollama, openai, local"
    exit 1
fi

if [ "$LLM_PROVIDER" = "ollama" ] || [ "$EMBEDDING_PROVIDER" = "ollama" ]; then
    USES_OLLAMA=true
fi

if [ "$LLM_PROVIDER" = "openai" ] || [ "$EMBEDDING_PROVIDER" = "openai" ]; then
    USES_OPENAI=true
fi

# Check required environment variables
if [ "$USES_OLLAMA" = true ] && [ -z "$OLLAMA_BASE_URL" ]; then
    echo "‚ùå OLLAMA_BASE_URL is required when LLM_PROVIDER or EMBEDDING_PROVIDER uses ollama"
    exit 1
fi

if [ "$USES_OPENAI" = true ] && [ -z "$OPENAI_API_KEY" ]; then
    echo "‚ùå OPENAI_API_KEY is required when LLM_PROVIDER or EMBEDDING_PROVIDER uses openai"
    exit 1
fi

if [ -z "$SECRET_KEY" ] || [ "$SECRET_KEY" = "your-secret-key-change-in-production" ]; then
    echo "‚ùå SECRET_KEY is not set or is using default value in .env file"
    exit 1
fi

# Check Ollama connectivity (only when needed)
if [ "$USES_OLLAMA" = true ]; then
    echo "üîó Checking Ollama connectivity..."
    if curl -f "${OLLAMA_BASE_URL}/api/tags" > /dev/null 2>&1; then
        echo "‚úÖ Ollama is accessible at ${OLLAMA_BASE_URL}"
    else
        echo "‚ùå Ollama is not accessible at ${OLLAMA_BASE_URL}"
        echo "   Please ensure Ollama is running and the models are available."
        exit 1
    fi
fi

echo "‚úÖ Environment configuration validated"

# Create necessary directories
mkdir -p logs
mkdir -p infrastructure/docker/init-scripts

# Start services with Docker Compose
echo "üê≥ Starting services with Docker Compose..."
docker-compose -f infrastructure/docker/docker-compose.yml up -d

# Wait for services to be ready
echo "‚è≥ Waiting for services to be ready..."
sleep 30

# Check service health
echo "üîç Checking service health..."

# Check API health
if curl -f http://localhost:8000/health/ > /dev/null 2>&1; then
    echo "‚úÖ API service is healthy"
else
    echo "‚ùå API service is not responding"
    echo "üìã Checking API logs..."
    docker-compose -f infrastructure/docker/docker-compose.yml logs api
    exit 1
fi

# Check PostgreSQL
if docker-compose -f infrastructure/docker/docker-compose.yml exec -T postgres pg_isready -U engram_user -d engram_db > /dev/null 2>&1; then
    echo "‚úÖ PostgreSQL is ready"
else
    echo "‚ùå PostgreSQL is not ready"
    exit 1
fi

# Check Redis
if docker-compose -f infrastructure/docker/docker-compose.yml exec -T redis redis-cli ping > /dev/null 2>&1; then
    echo "‚úÖ Redis is ready"
else
    echo "‚ùå Redis is not ready"
    exit 1
fi

# Check Neo4j
if curl -f http://localhost:7474 > /dev/null 2>&1; then
    echo "‚úÖ Neo4j is ready"
else
    echo "‚ùå Neo4j is not ready"
    exit 1
fi

echo ""
echo "üéâ Engram is now running!"
echo ""
echo "üìã Service URLs:"
echo "   ‚Ä¢ API Documentation: http://localhost:8000/docs"
echo "   ‚Ä¢ API Health Check: http://localhost:8000/health/"
echo "   ‚Ä¢ Flower (Celery): http://localhost:5555"
echo "   ‚Ä¢ Grafana (Metrics): http://localhost:3000 (admin/admin)"
echo "   ‚Ä¢ Prometheus: http://localhost:9090"
echo "   ‚Ä¢ Neo4j Browser: http://localhost:7474 (neo4j/secure_password)"
if [ "$USES_OLLAMA" = true ]; then
    echo "   ‚Ä¢ Ollama: ${OLLAMA_BASE_URL}"
fi
echo ""
echo "ü§ñ AI Providers:"
echo "   ‚Ä¢ LLM Provider: ${LLM_PROVIDER}"
echo "   ‚Ä¢ Embedding Provider: ${EMBEDDING_PROVIDER}"
if [ "$LLM_PROVIDER" = "ollama" ]; then
    echo "   ‚Ä¢ LLM Model: ${OLLAMA_LLM_MODEL:-gemma3:270m}"
elif [ "$LLM_PROVIDER" = "openai" ]; then
    echo "   ‚Ä¢ LLM Model: ${OPENAI_LLM_MODEL:-gpt-5-nano}"
fi

if [ "$EMBEDDING_PROVIDER" = "ollama" ]; then
    echo "   ‚Ä¢ Embedding Model: ${OLLAMA_EMBEDDING_MODEL:-nomic-embed-text:latest}"
elif [ "$EMBEDDING_PROVIDER" = "openai" ]; then
    echo "   ‚Ä¢ Embedding Model: ${OPENAI_EMBEDDING_MODEL:-text-embedding-3-small}"
elif [ "$EMBEDDING_PROVIDER" = "local" ]; then
    echo "   ‚Ä¢ Embedding Model: all-MiniLM-L6-v2 (local)"
fi
echo ""
echo "üîß Management Commands:"
echo "   ‚Ä¢ View logs: docker-compose -f infrastructure/docker/docker-compose.yml logs -f"
echo "   ‚Ä¢ Stop services: docker-compose -f infrastructure/docker/docker-compose.yml down"
echo "   ‚Ä¢ Restart services: docker-compose -f infrastructure/docker/docker-compose.yml restart"
echo ""
echo "üìö Next Steps:"
echo "   1. Visit http://localhost:8000/docs to explore the API"
echo "   2. Register a user account"
echo "   3. Start processing conversations and memories"
echo "   4. Monitor performance in Grafana"
echo ""
if [ "$USES_OLLAMA" = true ]; then
    echo "‚ö†Ô∏è  Make sure Ollama is running with the required models:"
    echo "   ollama pull ${OLLAMA_LLM_MODEL:-gemma3:270m}"
    echo "   ollama pull ${OLLAMA_EMBEDDING_MODEL:-nomic-embed-text:latest}"
    echo ""
fi

echo "Happy coding! üöÄ"
